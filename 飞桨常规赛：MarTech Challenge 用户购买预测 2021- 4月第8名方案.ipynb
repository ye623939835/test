{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data87787\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "  Using cached https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "data=pd.read_csv('data/data87787/train.csv')\r\n",
    "result=pd.read_csv('data/data87787/submission.csv')\r\n",
    "b=list(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['order_pay_time']=pd.to_datetime(data['order_pay_time'])\r\n",
    "##构建用户购买时间特征，包括上一次购买时间\r\n",
    "data1=data[data['order_pay_time']<='2013-07-31']\r\n",
    "data1 = data1.drop_duplicates(['order_id'])\r\n",
    "data1  = data1.set_index(['customer_id','order_id'])\r\n",
    "data1 = data1.sort_values(['customer_id','order_pay_time'])\r\n",
    "data1['last_pay_time'] = data1['order_pay_time'].shift()\r\n",
    "data1.reset_index(inplace=True)\r\n",
    "data1['customer_id_before'] = data1['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "def fill_row(x, y, a, b):\r\n",
    "    if x != y:\r\n",
    "        t = np.nan\r\n",
    "    else:\r\n",
    "        t = (a - b).days\r\n",
    "    return t\r\n",
    "data1['days_since_prior_order'] = data1.apply(lambda row: fill_row(row['customer_id'],row['customer_id_before'],row['order_pay_time'],row['last_pay_time']), axis=1)\r\n",
    "del data1['customer_id_before']\r\n",
    "del data1['last_pay_time']\r\n",
    "data1['order_hour_of_day'] = pd.DatetimeIndex(data1['order_pay_time']).hour\r\n",
    "data1['order_dow'] = pd.DatetimeIndex(data1['order_pay_time']).dayofweek\r\n",
    "data1['count']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##将购买时间分为工作时间（9-18点）记为1和非工作时间段记为0\r\n",
    "def datime(a):\r\n",
    "\tif a>9 and a<=18:\r\n",
    "\t\tt=1\r\n",
    "\telse:\r\n",
    "\t\tt=0\r\n",
    "\treturn t\r\n",
    "data2=data1[['customer_id','order_hour_of_day','count']]\r\n",
    "data2['work_time'] = data2.apply(lambda row: datime(row['order_hour_of_day']), axis=1)\r\n",
    "data2=data2.groupby(['customer_id','work_time']).agg({'count':'sum'})\r\n",
    "data2.reset_index(inplace=True)\r\n",
    "data2['lastid']=data2['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "data2['lastworktime']=data2['work_time'].shift().fillna(0).astype(np.uint32)\r\n",
    "data2['lastcount']=data2['count'].shift().fillna(0).astype(np.uint32)\r\n",
    "def qs(a,b,c,d,e,f):\r\n",
    "\tif a==b:\r\n",
    "\t\tif e>f:\r\n",
    "\t\t\tt=c\r\n",
    "\t\telse:\r\n",
    "\t\t\tt=d\r\n",
    "\telse:\r\n",
    "\t\tt=c\r\n",
    "\treturn t\r\n",
    "data2['work_time2'] = data2.apply(lambda row: qs(row['customer_id'],row['lastid'],row['work_time'],row['lastworktime'],row['count'],row['lastcount']), axis=1)\r\n",
    "data2=data2.groupby(['customer_id'])['work_time2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.DataFrame()\r\n",
    "train['customer_id']=data1['customer_id'].unique()\r\n",
    "train=train[['customer_id']].sort_values('customer_id')\r\n",
    "train['user_orderid_count'] = data1.groupby('customer_id')['order_id'].count().values \r\n",
    "train['user_days_since_prior_order_mean'] = data1.groupby('customer_id')['days_since_prior_order'].mean().values\r\n",
    "train['user_days_since_prior_order_max'] = data1.groupby('customer_id')['days_since_prior_order'].max().values\r\n",
    "train['user_days_since_prior_order_min'] = data1.groupby('customer_id')['days_since_prior_order'].min().values\r\n",
    "train['user_use_hour']=list(data2)\r\n",
    "del data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data3=data1[['customer_id','order_dow','count']]\r\n",
    "##将购买时间分为工作日（周一至周五）以及非工作日（周日周六）\r\n",
    "def datime2(a):\r\n",
    "\tif a>=1 and a<=5:\r\n",
    "\t\tt=1\r\n",
    "\telse:\r\n",
    "\t\tt=0\r\n",
    "\treturn t\r\n",
    "data3['week_time'] = data3.apply(lambda row: datime2(row['order_dow']), axis=1)\r\n",
    "data3=data3.groupby(['customer_id','week_time']).agg({'count':'sum'})\r\n",
    "data3.reset_index(inplace=True)\r\n",
    "data3['lastid']=data3['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "data3['lastweektime']=data3['week_time'].shift().fillna(0).astype(np.uint32)\r\n",
    "data3['lastcount']=data3['count'].shift().fillna(0).astype(np.uint32)\r\n",
    "def qs2(a,b,c,d,e,f):\r\n",
    "\tif a==b:\r\n",
    "\t\tif e>f:\r\n",
    "\t\t\tt=c\r\n",
    "\t\telse:\r\n",
    "\t\t\tt=d\r\n",
    "\telse:\r\n",
    "\t\tt=c\r\n",
    "\treturn t\r\n",
    "data3['week_time2'] = data3.apply(lambda row: qs2(row['customer_id'],row['lastid'],row['week_time'],row['lastweektime'],row['count'],row['lastcount']), axis=1)\r\n",
    "data3=data3.groupby(['customer_id'])['week_time2'].mean()\r\n",
    "train['user_use_week']=list(data3)\r\n",
    "del data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2=data1.groupby(['customer_id'])['order_total_num'].sum()\r\n",
    "train['order_total_num']=list(data2)\r\n",
    "del data2\r\n",
    "data2=data1.groupby('customer_id')['order_total_payment'].sum()\r\n",
    "train['order_total_pay']=list(data2)\r\n",
    "data2=data1.groupby('customer_id')['order_total_discount'].sum()\r\n",
    "train['discount']=list(data2)\r\n",
    "data2=data1.groupby(['customer_id'])['customer_gender'].max()\r\n",
    "train['sex']=list(data2)\r\n",
    "data2=data1.groupby('customer_id')['is_customer_rate'].mean()\r\n",
    "train['is_customer_rate']=list(data2)\r\n",
    "data2=data1[['customer_id','customer_province']]\r\n",
    "data2['customer_province']=pd.factorize(data1['customer_province'])[0]\r\n",
    "data2=data2.groupby('customer_id')['customer_province'].mean()\r\n",
    "train['province']=list(data2)\r\n",
    "train['user_days_since_prior_order_mean']=train['user_days_since_prior_order_mean'].fillna(1000)\r\n",
    "train['user_days_since_prior_order_max']=train['user_days_since_prior_order_max'].fillna(1000)\r\n",
    "train['user_days_since_prior_order_min']=train['user_days_since_prior_order_min'].fillna(1000)\r\n",
    "train['sex']=train['sex'].fillna(3)\r\n",
    "del data2\r\n",
    "del data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y=pd.DataFrame()\r\n",
    "train_y['customer_id']=train['customer_id'].unique()\r\n",
    "train_y['y']=0\r\n",
    "c=data[(data['order_pay_time']>='2013-08-1')&(data['order_pay_time']<'2013-08-31')]['customer_id'].unique()\r\n",
    "for i in range(len(train_y)):\r\n",
    "\tif train_y['customer_id'][i] in c:\r\n",
    "\t\ttrain_y['y'][i]=1\r\n",
    "train_y.set_index(['customer_id'], inplace=True)\r\n",
    "train.set_index(['customer_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\r\n",
    "model=xgb.XGBClassifier()\r\n",
    "#from sklearn.ensemble import RandomForestRegressor\r\n",
    "#model=RandomForestRegressor()\r\n",
    "model.fit(train,train_y)\r\n",
    "del train\r\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1=data\r\n",
    "data1 = data1.drop_duplicates(['order_id'])\r\n",
    "data1  = data1.set_index(['customer_id','order_id'])\r\n",
    "data1 = data1.sort_values(['customer_id','order_pay_time'])\r\n",
    "data1['last_pay_time'] = data1['order_pay_time'].shift()\r\n",
    "data1.reset_index(inplace=True)\r\n",
    "data1['customer_id_before'] = data1['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "def fill_row(x, y, a, b):\r\n",
    "    if x != y:\r\n",
    "        t = np.nan\r\n",
    "    else:\r\n",
    "        t = (a - b).days\r\n",
    "    return t\r\n",
    "data1['days_since_prior_order'] = data1.apply(lambda row: fill_row(row['customer_id'],row['customer_id_before'],row['order_pay_time'],row['last_pay_time']), axis=1)\r\n",
    "del data1['customer_id_before']\r\n",
    "del data1['last_pay_time']\r\n",
    "data1['order_hour_of_day'] = pd.DatetimeIndex(data1['order_pay_time']).hour\r\n",
    "data1['order_dow'] = pd.DatetimeIndex(data1['order_pay_time']).dayofweek\r\n",
    "data1['count']=1\r\n",
    "def datime(a):\r\n",
    "\tif a>9 and a<=18:\r\n",
    "\t\tt=1\r\n",
    "\telse:\r\n",
    "\t\tt=0\r\n",
    "\treturn t\r\n",
    "data2=data1[['customer_id','order_hour_of_day','count']]\r\n",
    "data2['work_time'] = data2.apply(lambda row: datime(row['order_hour_of_day']), axis=1)\r\n",
    "data2=data2.groupby(['customer_id','work_time']).agg({'count':'sum'})\r\n",
    "data2.reset_index(inplace=True)\r\n",
    "data2['lastid']=data2['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "data2['lastworktime']=data2['work_time'].shift().fillna(0).astype(np.uint32)\r\n",
    "data2['lastcount']=data2['count'].shift().fillna(0).astype(np.uint32)\r\n",
    "def qs(a,b,c,d,e,f):\r\n",
    "\tif a==b:\r\n",
    "\t\tif e>f:\r\n",
    "\t\t\tt=c\r\n",
    "\t\telse:\r\n",
    "\t\t\tt=d\r\n",
    "\telse:\r\n",
    "\t\tt=c\r\n",
    "\treturn t\r\n",
    "data2['work_time2'] = data2.apply(lambda row: qs(row['customer_id'],row['lastid'],row['work_time'],row['lastworktime'],row['count'],row['lastcount']), axis=1)\r\n",
    "data2=data2.groupby(['customer_id'])['work_time2'].mean()\r\n",
    "\r\n",
    "train=pd.DataFrame()\r\n",
    "train['customer_id']=data1['customer_id'].unique()\r\n",
    "train=train[['customer_id']].sort_values('customer_id')\r\n",
    "train['user_orderid_count'] = data1.groupby('customer_id')['order_id'].count().values \r\n",
    "train['user_days_since_prior_order_mean'] = data1.groupby('customer_id')['days_since_prior_order'].mean().values\r\n",
    "train['user_days_since_prior_order_max'] = data1.groupby('customer_id')['days_since_prior_order'].max().values\r\n",
    "train['user_days_since_prior_order_min'] = data1.groupby('customer_id')['days_since_prior_order'].min().values\r\n",
    "train['user_use_hour']=list(data2)\r\n",
    "del data2\r\n",
    "data3=data1[['customer_id','order_dow','count']]\r\n",
    "def datime2(a):\r\n",
    "\tif a>=1 and a<=5:\r\n",
    "\t\tt=1\r\n",
    "\telse:\r\n",
    "\t\tt=0\r\n",
    "\treturn t\r\n",
    "data3['week_time'] = data3.apply(lambda row: datime2(row['order_dow']), axis=1)\r\n",
    "data3=data3.groupby(['customer_id','week_time']).agg({'count':'sum'})\r\n",
    "data3.reset_index(inplace=True)\r\n",
    "data3['lastid']=data3['customer_id'].shift().fillna(0).astype(np.uint32)\r\n",
    "data3['lastweektime']=data3['week_time'].shift().fillna(0).astype(np.uint32)\r\n",
    "data3['lastcount']=data3['count'].shift().fillna(0).astype(np.uint32)\r\n",
    "def qs2(a,b,c,d,e,f):\r\n",
    "\tif a==b:\r\n",
    "\t\tif e>f:\r\n",
    "\t\t\tt=c\r\n",
    "\t\telse:\r\n",
    "\t\t\tt=d\r\n",
    "\telse:\r\n",
    "\t\tt=c\r\n",
    "\treturn t\r\n",
    "data3['week_time2'] = data3.apply(lambda row: qs2(row['customer_id'],row['lastid'],row['week_time'],row['lastweektime'],row['count'],row['lastcount']), axis=1)\r\n",
    "data3=data3.groupby(['customer_id'])['week_time2'].mean()\r\n",
    "train['user_use_week']=list(data3)\r\n",
    "del data3\r\n",
    "data2=data1.groupby(['customer_id'])['order_total_num'].sum()\r\n",
    "train['order_total_num']=list(data2)\r\n",
    "del data2\r\n",
    "data2=data1.groupby('customer_id')['order_total_payment'].sum()\r\n",
    "train['order_total_pay']=list(data2)\r\n",
    "data2=data1.groupby('customer_id')['order_total_discount'].sum()\r\n",
    "train['discount']=list(data2)\r\n",
    "data2=data1.groupby(['customer_id'])['customer_gender'].max()\r\n",
    "train['sex']=list(data2)\r\n",
    "data2=data1.groupby('customer_id')['is_customer_rate'].mean()\r\n",
    "train['is_customer_rate']=list(data2)\r\n",
    "data2=data1[['customer_id','customer_province']]\r\n",
    "data2['customer_province']=pd.factorize(data1['customer_province'])[0]\r\n",
    "data2=data2.groupby('customer_id')['customer_province'].mean()\r\n",
    "train['province']=list(data2)\r\n",
    "del data2\r\n",
    "del data1\r\n",
    "train['user_days_since_prior_order_mean']=train['user_days_since_prior_order_mean'].fillna(1000)\r\n",
    "train['user_days_since_prior_order_max']=train['user_days_since_prior_order_max'].fillna(1000)\r\n",
    "train['user_days_since_prior_order_min']=train['user_days_since_prior_order_min'].fillna(1000)\r\n",
    "train['sex']=train['sex'].fillna(3)\r\n",
    "train.set_index(['customer_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jj=model.predict(train)\r\n",
    "for i in range(len(result)):\r\n",
    "\tresult['result'][i]=jj[i]\r\n",
    "jj=pd.DataFrame(jj)\r\n",
    "jj.index=result['customer_id']\r\n",
    "jj.columns=['result']\r\n",
    "jj.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
